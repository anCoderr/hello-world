{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda with NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "print(device, 'with', device_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "all_characters = set(text)\n",
    "print(len(all_characters))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "encoder = {char: ind for ind, char in decoder.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[39 40 83 39 33 17  8 39 59 60]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "print(type(encoded_text))\n",
    "print(encoded_text[120:130])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "\n",
    "    return one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1,2,0])\n",
    "print(one_hot_encoder(arr, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "example_text = np.arange(10)\n",
    "print(example_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, sample_per_batch=10, seq_len=50):\n",
    "    char_per_batch = sample_per_batch * seq_len\n",
    "    num_batches_available = len(encoded_text)//char_per_batch\n",
    "    encoded_text = encoded_text[:num_batches_available*char_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape((sample_per_batch, -1))\n",
    "\n",
    "    for n in range(encoded_text.shape[1], seq_len):\n",
    "        x = encoded_text[:,n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:, n+seq_len]\n",
    "\n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:, 0]\n",
    "\n",
    "        yield x,y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(all_chars))\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "\n",
    "        lstm_output, hidden_state = self.lstm(x, hidden_state)\n",
    "\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "\n",
    "        return final_out, hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 5470292\n"
     ]
    }
   ],
   "source": [
    "model = CharModel(all_chars=all_characters, num_hidden=512, num_layers=3, drop_prob=0.5).to(device)\n",
    "total_params = []\n",
    "for p in model.parameters():\n",
    "    total_params.append(int(p.numel()))\n",
    "print('Total:', sum(total_params))\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "tracker = 0\n",
    "num_char = max(encoded_text)+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "train_percent = 0.9\n",
    "train_index = int(len(encoded_text) * train_percent)\n",
    "train_data = encoded_text[:train_index]\n",
    "test_data = encoded_text[train_index:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_state = hidden = (torch.zeros(model.num_layers,batch_size,model.num_hidden).to(device),torch.zeros(model.num_layers,batch_size,model.num_hidden).to(device))\n",
    "\n",
    "    for x, y in generate_batches(train_data, batch_size, seq_len):\n",
    "\n",
    "        tracker += 1\n",
    "\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "\n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        targets = torch.from_numpy(y).to(device)\n",
    "\n",
    "        hidden = ([state.data])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}